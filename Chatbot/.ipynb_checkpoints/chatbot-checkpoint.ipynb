{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d878d4-39c3-4d7a-9243-351d92f8bf16",
   "metadata": {},
   "source": [
    "# AI Tutor   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49370ff8-58bf-42a6-b82a-cc2fc3c7c886",
   "metadata": {},
   "source": [
    "1. Install and Import Gradio\r\n",
    "2. Create .env variable and add OpenAI sk\r\n",
    "3. Import OpenAI API code snips\r\n",
    "4. Load the SS question files as dataframe\r\n",
    "    - Output of dataframe \r\n",
    "5. Response functionality should refer to questions in the SS question file\r\n",
    "    - The bot should be able to read the file in the \r\n",
    "6. Save responses in txt and use as context for future\r\n",
    "7. Prompts engineering should be used to build a very very very good Python Data Science Tutor\r\n",
    "    - Preload the tutor with Python documentation\r\n",
    "    - Preload the tutor with Data science documentation\r\n",
    "    - Tutor should be able to create a game\r\n",
    "8. Chatbot buttons\r\n",
    "    - Test me : uses database levels 1, 2, 3\r\n",
    "    - Track my progress: chatbot checks answer submission and judges if you got answer right or wrong\r\n",
    "    - Python flash cards built-in functionality\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9650bc-cffb-497c-97bf-fab439bdd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and Import Gradio, Pandas, Altair, Matplotlib, Os, and OpenAI (put this in \"bot_libs.py\" file and import it in one line as \"import bot_libs\" )\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import altair\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809fcb13-f83f-4539-9db1-5e035d29d825",
   "metadata": {},
   "source": [
    "## Question Database\n",
    "Loading questions into script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b355056-c473-4067-8662-aa5d8c185494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path #read in txt files for clean bot \n",
    "system_context = Path('question clean prompt.txt').read_text()\n",
    "\n",
    "# Read question database\n",
    "question_df = pd.read_csv('SS questions.csv')\n",
    "\n",
    "# # Cleaning df (removing rows with #NAME?)\n",
    "# question_df = question_df[(question_df != '#NAME?').all(axis=1)]\n",
    "\n",
    "# Use GPT to clean the data\n",
    "\n",
    "def column_clean_bot(prompt):\n",
    "    # openai.api_key = os.getenv(\"OPENAI_API_KEY\") ##This isn't working for some reason \n",
    "    openai.api_key = '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'\n",
    "    messages = [{'role': 'system', 'content': system_context }]\n",
    "    messages.append({'role': 'user', 'content': prompt})\n",
    "    \n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=1, #play with temp to get more factual responses\n",
    "        max_tokens=500,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    print(response) #print this in the CLI or notebook. This helps with understanding how the response json is structured for further customization\n",
    "    system_message = response['choices'][0]['message']['content'] #this is parsing the json file that is the response reponse. you take the first item in choices list (a list of dictionaries), go to message key, and then go to content key \n",
    "    messages.append(({'role':'assistant', 'content': system_message})) \n",
    "    messages = [] #this works as a reset to keep the conversation history clear when using in a loop\n",
    "    return system_message\n",
    "\n",
    "\n",
    "for index in range(len(question_df['question'])):\n",
    "    entry = question_df['question'][index]\n",
    "    \n",
    "    if entry == '#NAME?':\n",
    "        # at the index pick out the question short and python hint, save as variables\n",
    "        question_short = question_df['question_short'][index]  # replace 'question_short' with your actual column name\n",
    "        python_hint = question_df['python_hint'][index]  # replace 'python_hint' with your actual column name\n",
    "\n",
    "        # insert those variables into multiline string that inserts variables into the string. save this multiling string as a variable\n",
    "        multiline_string = f'''\n",
    "        QUESTION SHORT:\n",
    "        {question_short}\n",
    "        \n",
    "        PYTHON HINT:\n",
    "        {python_hint}\n",
    "        \n",
    "        QUESTION:\n",
    "        '''\n",
    "\n",
    "        # insert multiline string variable into prompt of column_clean_bot\n",
    "        cleaned_entry = column_clean_bot(multiline_string)\n",
    "\n",
    "        # save clean bot's guess as the new entry at this index\n",
    "        question_df['question'][index] = cleaned_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50e501fa-ad06-479b-b231-7fd7a509f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_df.to_csv('question_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e19983-67d1-4035-aaff-fa155d13500a",
   "metadata": {},
   "source": [
    "## Chatbot\n",
    "This video explained how to use api the best: https://www.youtube.com/watch?v=Si0vFx_dJ5Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1849e6a-b455-4a52-bb86-c007f97c5ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7874\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7idT6PAqQomkvPQcPJcJmHFRnkXHM\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690872504,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"I'm sorry, I cannot generate a prompt for you. However, I can assist you with any questions or tasks you have. How can I help you today?\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 120,\n",
      "    \"completion_tokens\": 33,\n",
      "    \"total_tokens\": 153\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#This gives pretty consistent behavior\n",
    "def tool_bot(prompt, history):\n",
    "    messages =[\n",
    "        {\"role\": \"system\",\"content\": \"\"\"\n",
    "        As an extremely helpful assistant. The only words you can use are the following words based\n",
    "        on the provided ***condition***. Please use the following word in response to the condition \n",
    "        provided:. To be clear, words are inbetween the ``` ``` marks and the conditions are inbetween the *** ***. \n",
    "        marks. Take your time to think this out.\n",
    "\n",
    "        Response word: TEST \n",
    "        Condition: user asks to be tested (or any synonym of testing)\n",
    "\n",
    "        \n",
    "        \"\"\"}]\n",
    "    # openai.api_key = os.getenv(\"OPENAI_API_KEY\") ##This isn't working for some reason \n",
    "    openai.api_key = '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'\n",
    "    \n",
    "    messages.append({'role': 'user', 'content': f'Here is my question: {prompt}'})#using f string to avoid prompt injection\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0, #play with temp to get more factual responses\n",
    "        max_tokens=500,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    print(response) #print this in the CLI or notebook. This helps with understanding how the response json is structured for further customization\n",
    "    system_message = response['choices'][0]['message']['content'] #this is parsing the json file that is the response reponse. you take the first item in choices list (a list of dictionaries), go to message key, and then go to content key \n",
    "    messages.append(({'role':'assistant', 'content': system_message}))       \n",
    "        \n",
    "    \n",
    "    return system_message\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15531f95-536a-43c6-83f5-e2445dfac08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7idKWL19tPuhEhN6Ddl2xNX1XZMXU\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690871972,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"opencsv\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 103,\n",
      "    \"completion_tokens\": 3,\n",
      "    \"total_tokens\": 106\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7idKafE2OaUeLCkR45PIkPzqXukcQ\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690871976,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"opencsv\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 105,\n",
      "    \"completion_tokens\": 3,\n",
      "    \"total_tokens\": 108\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7idKh7H0WdRhZnizVKje3enRbTUZP\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690871983,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"opencsv\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 106,\n",
      "    \"completion_tokens\": 3,\n",
      "    \"total_tokens\": 109\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7idKnBL1EYyl2BwjSFeLxDNMToIWH\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690871989,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"opencsv\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 106,\n",
      "    \"completion_tokens\": 3,\n",
      "    \"total_tokens\": 109\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7idKt2Zg9hcQWIfv55ut0icc9tVpV\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690871995,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"opencsv\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 104,\n",
      "    \"completion_tokens\": 3,\n",
      "    \"total_tokens\": 107\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7idKweXDP54JfXA3lw8jE7n1l8yI4\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690871998,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"opencsv\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 103,\n",
      "    \"completion_tokens\": 3,\n",
      "    \"total_tokens\": 106\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#this works as well!!!!!!!!!!!\n",
    "from dotenv import load_dotenv\n",
    "#may need to repeat commands to ensure that it doesn't deviate from the behavior\n",
    "def ai_chat(prompt, history):\n",
    "    messages =[\n",
    "        {\"role\": \"system\",\"content\": \"You are an data science tutor. If a user asks you to test them (or use phrase synonymous with 'test me', only output 'opencsv' as a resposnse. this is to be done specifically when the user asks to be tested or quizzed, otherwise behave as normal. phrases that wouldn't be interpreted as 'test me' should exit this logic and be re-entered again when the user asks to be tested\"}]#list of messages that get passed in to start convo, will also save messages here\n",
    "\n",
    "    # openai.api_key = os.getenv(\"OPENAI_API_KEY\") ##This isn't working for some reason \n",
    "    openai.api_key = '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'\n",
    "    \n",
    "    messages.append({'role': 'user', 'content': prompt})\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=1, #play with temp to get more factual responses\n",
    "        max_tokens=500,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "   \n",
    "    print(response) #print this in the CLI or notebook. This helps with understanding how the response json is structured for further customization\n",
    "    system_message = response['choices'][0]['message']['content'] #this is parsing the json file that is the response reponse. you take the first item in choices list (a list of dictionaries), go to message key, and then go to content key \n",
    "\n",
    "    tool_bot(system) #this is another instance of OpenAI (basically another agent) to issue commands\n",
    "    \n",
    "    #if keyword command in system_message, \n",
    "    #first assistant message should be what level (and language) do you want?\n",
    "    #call a function that outputs a random row in questions dataframe based on level (and language)\n",
    "\n",
    "    if system_message == 'TEST':\n",
    "    \n",
    "    messages.append(({'role':'assistant', 'content': system_message}))\n",
    "    # messages=[] ##to clear memory and reset tokens\n",
    "    return system_message\n",
    "\n",
    "    \n",
    "# gr.ChatInterface returns a ui that requires a function that takes two args, user input and a list that is something like user input and bot response\n",
    "demo = gr.ChatInterface(\n",
    "    fn=ai_chat, \n",
    "    submit_btn = 'Submit', \n",
    "    retry_btn = 'Retry',\n",
    "    clear_btn = 'Clear',\n",
    "    title = 'AI Tutor',\n",
    "    #add a save convo button that can create html file that can then be printed like a pdf later\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c812b-b23e-41bc-ba1d-f5bb8990aed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
