{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d878d4-39c3-4d7a-9243-351d92f8bf16",
   "metadata": {},
   "source": [
    "# AI Tutor\n",
    "1. Install and Import Gradio\n",
    "2. create .env variable and add OpenAI sk\n",
    "3. Import OpenAI API code snips\n",
    "4. Load the SS question files as dataframe\n",
    "    - Output of dataframe \n",
    "6. Response functionality should refer to questions in the SS question file\n",
    "    - the bot should be able to read the file in the \n",
    "7. Save responses in txt and use as context for future\n",
    "8. Prompts engineering should be used to build a very very very good Python Datascience Tutor\n",
    "    - Preload the tutor with Python documentation\n",
    "    - Preload the tutor with Data science documentation\n",
    "    - Tutor should be able to create a game\n",
    "9. Chatbot buttons\n",
    "    - test me : uses database levels 1, 2, 3\n",
    "    - track my progress: chatbot checks answer submission and judges if you got answer right or wrong\n",
    "    - python flash cards built-in functionality\n",
    "    - \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9650bc-cffb-497c-97bf-fab439bdd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and Import Gradio, Pandas, Altair, Matplotlib, Os, and OpenAI (put this in \"bot_libs.py\" file and import it in one line as \"import bot_libs\" )\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import altair\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caaf8a44-ff09-4a5d-bd99-3be85d010c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matte\\AppData\\Local\\Temp\\ipykernel_8744\\3343720030.py:21: GradioUnusedKwargWarning: You have unused kwarg parameters in Interface, please remove them: {'state': state}\n",
      "  demo = gr.Interface(\n",
      "C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\utils.py:833: UserWarning: Expected 2 arguments for function <function CustomChatGPT at 0x0000018C793FDD80>, received 1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\utils.py:837: UserWarning: Expected at least 2 arguments for function <function CustomChatGPT at 0x0000018C793FDD80>, received 1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\helpers.py:710: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 442, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1392, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1097, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\utils.py\", line 703, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\matte\\AppData\\Local\\Temp\\ipykernel_8744\\3343720030.py\", line 5, in CustomChatGPT\n",
      "    state['messages'].append({'role': 'user', 'content' : user_input})\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 442, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1392, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1097, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\utils.py\", line 703, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\matte\\AppData\\Local\\Temp\\ipykernel_8744\\3343720030.py\", line 5, in CustomChatGPT\n",
      "    state['messages'].append({'role': 'user', 'content' : user_input})\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 442, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1392, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1097, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\utils.py\", line 703, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\matte\\AppData\\Local\\Temp\\ipykernel_8744\\3343720030.py\", line 5, in CustomChatGPT\n",
      "    state['messages'].append({'role': 'user', 'content' : user_input})\n",
      "TypeError: 'NoneType' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def CustomChatGPT(user_input, state):\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    # Add the new user message to the chat history\n",
    "    state['messages'].append({'role': 'user', 'content' : user_input})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = 'gpt-3.5-turbo',\n",
    "        messages = state['messages']\n",
    "    )\n",
    "\n",
    "    # Add the model's response to the chat history\n",
    "    ChatGPT_reply = response['choices'][0]['message']['content']\n",
    "    state['messages'].append({'role':'assistant' , 'content' : ChatGPT_reply})\n",
    "\n",
    "    # Convert the chat history into a string\n",
    "    chat_history = '\\n'.join([f\"{msg['role']}: {msg['content']}\" for msg in state['messages']])\n",
    "\n",
    "    return chat_history, state\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=CustomChatGPT, \n",
    "    inputs = 'text', \n",
    "    outputs = 'text', \n",
    "    title = 'AI Tutor',\n",
    "    state=gr.State({'messages': []})\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b355056-c473-4067-8662-aa5d8c185494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This works! work from here\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "messages = []\n",
    "\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# openai.api_key = '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'\n",
    "def CustomChatGPT(user_input):\n",
    "    messages.append({'role': 'user','content' : user_input})\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = 'gpt-3.5-turbo',\n",
    "        messages = messages\n",
    "    )\n",
    "    ChatGPT_reply = response['choices'][0]['message']['content']\n",
    "    messages.append({'role':'assistant' , 'content' : ChatGPT_reply})\n",
    "    return ChatGPT_reply\n",
    "\n",
    "demo = gr.Interface(fn=CustomChatGPT, inputs = 'text', outputs = 'text', title = 'AI Tutor')\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15531f95-536a-43c6-83f5-e2445dfac08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7876\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7hxT7Hv79NcpVvgFXDn9T8kNXr4OQ\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690711057,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Hello! How can I help you today?\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 19,\n",
      "    \"completion_tokens\": 9,\n",
      "    \"total_tokens\": 28\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7hxTPqwZgcsl4ta741XSrTMzRSqjo\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690711075,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Of course! Here are some tips to help you in your data science journey:\\n\\n1. Learn and understand the basics: Start with a strong foundation in programming languages like Python or R, and familiarize yourself with key concepts in statistics, linear algebra, and calculus.\\n\\n2. Gain practical experience: Work on real-world projects and datasets to apply your skills and gain hands-on experience. Participate in Kaggle competitions or contribute to open-source projects.\\n\\n3. Master data manipulation and preprocessing: A significant portion of data science work involves cleaning and transforming data. Learn how to handle missing values, outliers, and apply techniques like feature scaling, one-hot encoding, and normalization.\\n\\n4. Develop strong exploratory data analysis skills: Use visualization tools and techniques to understand the data distribution, identify patterns, and uncover insights. This will help in feature selection and building predictive models.\\n\\n5. Build a solid foundation in machine learning: Understand the algorithms and techniques used in machine learning. Experiment with various models like linear regression, decision trees, random forests, and deep learning models like neural networks.\\n\\n6. Evaluate and fine-tune models: Learn how to assess model performance using metrics like accuracy, precision, recall, and F1-score. Practice techniques like cross-validation, hyperparameter tuning, and ensemble methods\"\n",
      "      },\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 43,\n",
      "    \"completion_tokens\": 256,\n",
      "    \"total_tokens\": 299\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "messages =[\n",
    "        {\"role\": \"system\",\"content\": \"You are an data science tutor.\"}]#list of messages that get passed in to start convo, will also save messages here\n",
    "def ai_chat(prompt, history):\n",
    "    # openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    openai.api_key = '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'\n",
    "    \n",
    "    messages.append({'role': 'user', 'content': prompt})\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=1,\n",
    "        max_tokens=256,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    print(response)\n",
    "    system_message = response['choices'][0]['message']['content'] #this is parsing the json file that is the response reponse. you take the first item in choices list (a list of dictionaries), go to message key, and then go to content key \n",
    "    messages.append(({'role':'assistant', 'content': system_message}))\n",
    "    return system_message\n",
    "\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=ai_chat, \n",
    "    submit_btn = 'Submit', \n",
    "    retry_btn = 'Retry',\n",
    "    clear_btn = 'Clear',\n",
    "    title = 'AI Tutor',\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c812b-b23e-41bc-ba1d-f5bb8990aed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
