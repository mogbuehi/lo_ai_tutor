{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d878d4-39c3-4d7a-9243-351d92f8bf16",
   "metadata": {},
   "source": [
    "# AI Tutor   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49370ff8-58bf-42a6-b82a-cc2fc3c7c886",
   "metadata": {},
   "source": [
    "1. Install and Import Gradio\r\n",
    "2. Create .env variable and add OpenAI sk\r\n",
    "3. Import OpenAI API code snips\r\n",
    "4. Load the SS question files as dataframe\r\n",
    "    - Output of dataframe \r\n",
    "5. Response functionality should refer to questions in the SS question file\r\n",
    "    - The bot should be able to read the file in the \r\n",
    "6. Save responses or csv in txt and use as context for sessions future\r\n",
    "7. Prompts engineering should be used to build a very very very good Python Data Scieneate a game\r\n",
    "8. Chatbot buttons\r\n",
    "    - Test me : uses database levels 1, 2, 3\r\n",
    "    - Track my progress: chatbot checks answer submission and judges if you got answer right or wrong\r\n",
    "    - Python flash cards built-in functionality\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9650bc-cffb-497c-97bf-fab439bdd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and Import Gradio, Pandas, Altair, Matplotlib, Os, and OpenAI (put this in \"bot_libs.py\" file and import it in one line as \"import bot_libs\" )\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "from pathlib import Path #read in txt files for clean bot \n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "190f4ce3-8873-4d62-8e6e-e35f1a0d8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a list of files and directories in the specified path...used during troubleshooting\n",
    "\n",
    "# os.getcwd()\n",
    "# os.chdir('C:\\\\Users\\\\matte\\\\OneDrive\\\\Desktop\\\\Lonely Octopus\\\\Freelance Project\\\\Chatbot\\\\version 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e501fa-ad06-479b-b231-7fd7a509f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Question DF\n",
    "question_df = pd.read_csv('question_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1849e6a-b455-4a52-bb86-c007f97c5ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool bot is designed to be the tool-using feature of the chat bot. \n",
    "# During the session it will be a second instance of GPT-3.5 that can use tools\n",
    "# This gives pretty consistent behavior\n",
    "with open('tool bot sys content.txt', 'r') as file:\n",
    "    tool_bot_sys_content = file.read()\n",
    "\n",
    "def tool_bot(prompt, history=[]):\n",
    "    messages =[\n",
    "        {\"role\": \"system\", \"content\": tool_bot_sys_content}\n",
    "    ]\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\") ##This isn't working for some reason \n",
    "    messages.append({'role': 'user', 'content': f'Here is my question or statement: {prompt}'}) # using f string to avoid prompt injections\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0, #play with temp to get more factual responses\n",
    "        max_tokens=10,#further limit output\n",
    "        top_p=0,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "\n",
    "    # saving the message we want to show to the user\n",
    "    assistant_content = response['choices'][0]['message']['content']\n",
    "\n",
    "    # Visual check for testing purposes\n",
    "    print(response) \n",
    "\n",
    "    return assistant_content\n",
    "   \n",
    "# #testing tool_bot\n",
    "\n",
    "# prompt = input()\n",
    "# tool_bot(prompt=prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57b3c351-66af-4202-8b1f-fc3c384674e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matte\\AppData\\Local\\Temp\\ipykernel_22868\\541175320.py:17: DeprecationWarning: Firefox will soon stop logging to geckodriver.log by default; Specify desired logs with log_output\n",
      "  service = FirefoxService(executable_path=GECKODRIVER_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of the h1 tag: Most Profitable Companies\n",
      "Most Profitable Companies\n"
     ]
    }
   ],
   "source": [
    "#html parser bot\n",
    "# bot should take url, grab certain data from the website\n",
    "# should also be able to see if you failed the question and see what you submitted\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Function that takes url as input\n",
    "def html_scraper(url):\n",
    "\n",
    "    # Path to the GeckoDriver executable\n",
    "    GECKODRIVER_PATH = 'geckodriver.exe'\n",
    "    \n",
    "    # Initialize the WebDriver\n",
    "    options = webdriver.FirefoxOptions()\n",
    "    service = FirefoxService(executable_path=GECKODRIVER_PATH)\n",
    "    driver = webdriver.Firefox(service=service, options=options)\n",
    "    \n",
    "    # Open a web page\n",
    "    # url = input('url')\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for JavaScript to load content (adjust time as needed)\n",
    "    time.sleep(5)  # Wait for 5 seconds\n",
    "    \n",
    "    # Get HTML content\n",
    "    html_content = driver.page_source\n",
    "    \n",
    "    # # Parse HTML content with Beautiful Soup...used to get all tags and explore data\n",
    "    # soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # for tag in soup.find_all(True):\n",
    "    #     print(f\"Tag: {tag.name}\")\n",
    "    #     print(f\"Attributes: {tag.attrs}\")\n",
    "    #     content = str(tag.contents).strip()\n",
    "    #     print(f\"Content: {content[:100]}...\") if len(content) > 100 else print(f\"Content: {content}\")\n",
    "    #     print(\"-\" * 50) # Separator line\n",
    "    # ... (other code)\n",
    "    \n",
    "    # Get HTML content\n",
    "    html_content = driver.page_source\n",
    "    \n",
    "    # Parse HTML content with Beautiful Soup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find the first h1 tag\n",
    "    h1_tag = soup.find('h1')\n",
    "    \n",
    "    # Print the content if the h1 tag exists\n",
    "    if h1_tag:\n",
    "        print(\"Content of the h1 tag:\", h1_tag.text)\n",
    "    else:\n",
    "        print(\"No h1 tag found on the page.\")\n",
    "    \n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "    return h1_tag.text\n",
    "\n",
    "url=' https://platform.stratascratch.com/coding/10354-most-profitable-companies?code_type=2'\n",
    "# url = input('url')\n",
    "question_short = html_scraper(url)\n",
    "print(question_short)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1badf1-c86b-40da-9dcf-6f38d0d783a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Find the 3 most profitable companies in the entire world.\n",
      " -- Output the result along with the corresponding company name.\n",
      " -- Sort the result based on profits in descending order.\n",
      "\n",
      "- Use **rank()** function to ensure an edge cases.\n",
      "\n",
      "- Use sort_values(column_name, order) to sort along a specified column; Set order to False to display the printed values in descending order \n",
      " - Limit rows to be printed by specifying rank on profits.\n",
      " - Use [ [ column_name/s] ] to return a specified column of the dataframe\n",
      "\n",
      "SELECT company,\n",
      "  profit\n",
      " FROM\n",
      "  (SELECT *,\n",
      "  rank() OVER (\n",
      "  ORDER BY profit DESC) as rank\n",
      "  FROM\n",
      "  (SELECT company,\n",
      "  sum(profits) AS profit\n",
      "  FROM forbes_global_2010_2014\n",
      "  GROUP BY company) sq) sq2\n",
      " WHERE rank <=3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pulling out the question row\n",
    "question_row = question_df.loc[question_df['question_short'] == question_short]\n",
    "# print(question_row)\n",
    "\n",
    "# convert DataFrame to numpy array to list in order to extract the string objects contained inside\n",
    "hint = question_row['hint'].values.tolist()\n",
    "py_hint = question_row['python_hint'].values.tolist()\n",
    "question = question_row['question'].values.tolist()\n",
    "py_solution = question_row['solution'].values.tolist()\n",
    "\n",
    "print(f'''{question[0]}\n",
    "\n",
    "{hint[0]}\n",
    "\n",
    "{py_hint[0]}\n",
    "\n",
    "{py_solution[0]}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15531f95-536a-43c6-83f5-e2445dfac08e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7875\n",
      "Running on public URL: https://5b7a8c0d5512a103af.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5b7a8c0d5512a103af.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The AI Tutor Backend. It has a nested tool_bot function\n",
    "# Which will trigger grabbing data from questions database\n",
    "with open('ai tutor sys content.txt', 'r') as file:\n",
    "    ai_tutor_sys_content = file.read()\n",
    "\n",
    "# Global messages list. The memory of the conversation is stored in this list\n",
    "messages =[{\"role\": \"system\",\"content\": ai_tutor_sys_content + f'''\n",
    "Here is the context to consider as you help the student:\n",
    "Question: \n",
    "{question[0]}\n",
    "\n",
    "Hint: \n",
    "{hint[0]}\n",
    "\n",
    "Python Hint (code snippet):\n",
    "{py_hint[0]}\n",
    "\n",
    "Solution: \n",
    "{py_solution[0]} '''}]\n",
    "\n",
    "#track index of conversation:\n",
    "i = [0]\n",
    "\n",
    "denial_messages = messages #creating a separate history \n",
    "load_dotenv()\n",
    "def ai_chat(prompt, history):\n",
    "    # calling messages variable inside the function\n",
    "    global messages, denial_messages\n",
    "            \n",
    "    tool_response = tool_bot(prompt) #this is another instance of OpenAI (basically another agent) to issue commands\n",
    "    print(f'TOOL RESPONSE: {tool_response}')\n",
    "    \n",
    "    # Using tool_bot to analyze conversation and to respond accordingly. \n",
    "    # tool_response is\n",
    "    if tool_response == 'OFF TOPIC' or tool_response == 'CAREER ADVICE':\n",
    "        print(\"DENIAL LOGIC ACTIVATED\")\n",
    "        denial_messages.append({'role': 'user', 'content': f'''This is my prompt:\n",
    "        \n",
    "        Prompt: \"\"\" {prompt} \"\"\"\n",
    "        \n",
    "        Kindly tell me to stay on the topic of python and data science and/or to avoid questions about career advice\n",
    "        explain to me why my previous prompt was off topic in a sentence or 2.'''})\n",
    "        response = openai.ChatCompletion.create(\n",
    "             model=\"gpt-3.5-turbo\",\n",
    "             messages=denial_messages, #using this message histroy because I don't want to save this to convo history\n",
    "             temperature=1, #play with temp to get more factual responses\n",
    "             max_tokens=100,\n",
    "             top_p=1,\n",
    "             frequency_penalty=0,\n",
    "             presence_penalty=0\n",
    "         )\n",
    "        token_count = response['usage']['total_tokens']\n",
    "        if token_count >= 3000:\n",
    "            denial_messages = [denial_messages[0]]\n",
    "        assistant_content = response['choices'][0]['message']['content']\n",
    "        # not appending message history\n",
    "        return assistant_content \n",
    " \n",
    "    # tool_dict = {}\n",
    "    # for tool in tool_dict:\n",
    "    #     if tool_response == tool:\n",
    "    #         assistant_content = tool_dict[tool]\n",
    "    #         return assistant_content\n",
    "            \n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\") ##This isn't working for some reason \n",
    "    messages.append({'role': 'user', 'content': f'{prompt}'}) #f string to avoid prompt injection errors from user\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=1, #play with temp to get more factual responses\n",
    "        max_tokens=500,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "\n",
    "    assistant_content = response['choices'][0]['message']['content']\n",
    "    # hint = '' #this is parsing the json file that is the response reponse. you take the first item in choices list (a list of dictionaries), go to message key, and then go to content key \n",
    "    \n",
    "    #print this in the CLI or notebook. This helps with understanding how the response json is structured for further customization\n",
    "    print(f'''\n",
    "    \n",
    "    MESSAGE: {messages}\n",
    "    \n",
    "    RESPONSE:{response}\n",
    "    \n",
    "    ''')\n",
    "    \n",
    "    # Saving as variable to store the conversation history\n",
    "    format_str = f'''\n",
    "    User: {prompt}\n",
    "    \n",
    "    Response: {assistant_content}\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Saving content to a text file\n",
    "    # Open a file in write mode, 'a' argument serves as appending to\n",
    "    with open('saved_chat.txt', 'a') as file:\n",
    "        file.write(format_str)\n",
    "\n",
    "    #add token counter and a way to handle hitting token limit.  \n",
    "    # Prob summarizing previous assistant/student info into 2-4 sentences, 3000 tokens is safe place to start summary\n",
    "    token_count = response['usage']['total_tokens']\n",
    "    if token_count >= 3000:\n",
    "        # Reload message history text for summarization. Better formatted than parsing the response JSON\n",
    "        # content_txt = Path('saved_chat.txt').read_text() \n",
    "        # summary_str[0] = summarize_bot_saved(content_txt)\n",
    "        messages = [messages[0]] # basically erase the memory except the system message and kept it as a list\n",
    "        # print(f'This is the `messages` memory: {summary_str}')\n",
    "    \n",
    "    messages.append(({'role':'assistant', 'content': assistant_content}))\n",
    "    absolute_path = os.path.abspath('matt/Most Profitable Companies faq.html')\n",
    "    print(absolute_path)\n",
    "    \n",
    "    return assistant_content\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    \n",
    "    fn=ai_chat, \n",
    "    submit_btn = 'Submit', \n",
    "    retry_btn = 'Retry',\n",
    "    clear_btn = 'Clear',\n",
    "    title = 'AI Backend V1',\n",
    "    #add a save convo button that can create html file that can then be printed like a pdf later\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae92e0-055a-4c3d-9592-d4d0baca541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' simplify database, only save the question_short and the solution. Bot then parses html of current page user is on in \n",
    "order to look up what question and the solution are as context, and dynamically generates hints and walkthrough. \n",
    "This can be saved to a csv for future reference as well. THIS FEATURE WOULD NEED TO BE BUILT ON THE STATASCRATCH SIDE. IN INTEREST OF TIME\n",
    "WE COULD ONLY DEMO WHAT IT WOULD LOOK LIKE WHILE NAVAGATING A PAGE ON THE WEBSITE. ALSO LOOK TO INTEGRATE THE SCROLL DOWN MENU TO SELECT WHICH QUESTION YOURE ON\n",
    "\n",
    "Copy paste the url and bot will scrape the data everytime the user prompts the bot to get the current state. \n",
    "INSTEAD GO FOR USING THE CONTEXT APPROACH, APP SHOULD BE USED TO GIVE WALKTHROUGH, EDGE CASE, AND GENERAL CODING ASSISTANCE,\n",
    "USING THE CURRENT QUESTION AS CONTEXT. when you click AI tutor button, it should know which question you are on. \n",
    "\n",
    "The pseudocode on the backend should look something like\n",
    "if button pressed:\n",
    "    check user's current url\n",
    "    select row in database the corresponds to that question url\n",
    "    feed in data from that row as context for the tutor\n",
    "    when \"Walkthrough\" or \"Explain Solution\" button is pressed it takes the database as context and uses LLM pretraining to give walkthrough or explain the solution\n",
    "    \n",
    "\n",
    "can also save all conversations that can be analyzed for future referece by the team at statascratch. Can use an LLM to\n",
    "then categorize types of questions asked and perform analytics.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9444ca52-2c31-4c04-be07-650b0d5eafbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9155b36-a8a4-4177-b329-bc081bec1dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c812b-b23e-41bc-ba1d-f5bb8990aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add functionality that tracks tokens and keeps system message, but deletes or summarizes conversation into a few lines, and feeds resets token count back to where it was at beginning of convo.\n",
    "#FAQ database:\n",
    "\n",
    "#Save responses, create an openai instance that can read each entry column one at atime (to avoid token limit)\n",
    "# and categorize commonly asked responses...and save that as a csv for future analysis\n",
    "\n",
    "#Future functionality:\n",
    "#Human like responsiveness. randomized delayed timer\n",
    "#Voice optionality. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c6bcb-da39-408a-a0cb-05f494c35d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
