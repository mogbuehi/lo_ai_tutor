{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b916bdb9-aff7-4847-80d5-d3951f2b5375",
   "metadata": {},
   "source": [
    "## AI Tutor\n",
    "- Upload csv with questions and generate and show\n",
    "    - Hints\n",
    "    - Walkthrough\n",
    "    - Solutions\n",
    "    - Edge Cases\n",
    " \n",
    "- UI\n",
    "- Integration ideas\n",
    "- Save chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "249ce119-4047-4129-ad64-532bd71a9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "# ---------------------------------------------------\n",
    "# UI\n",
    "import gradio as gr\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# plotting libs\n",
    "import altair as alt\n",
    "import matplotlib as plt\n",
    "\n",
    "# os and env libs\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path #read in txt files for clean bot \n",
    "\n",
    "# OpenAI python lib\n",
    "import openai\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "775f705f-84dc-48e8-a35e-f5a239f16797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_tutor chat function\n",
    "# when asked a python question, it should be able to create walkthrough, hints, \n",
    "\n",
    "# Load OpenAI API key:\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# read in context as a string and save it to `messages`, a list that will store all messages from session (up to token limit)\n",
    "ai_tutor_sys_content = Path('ai_tutor_sys_content.txt').read_text()\n",
    "messages =[{\"role\": \"system\",\"content\": ai_tutor_sys_content}]\n",
    "\n",
    "index = [0] # a way to save the state of conversation\n",
    "\n",
    "# ai_tutor fxn\n",
    "def ai_tutor(prompt, history):\n",
    "\n",
    "    # Should I start with an assistant message to help the user get started?]\n",
    "    # List of messages that get passed in to start convo, will also save messages here\n",
    "    # f string to avoid prompt injection errors from user\n",
    "    messages.append({'role': 'user', 'content': f'''user prompt:\n",
    "    {prompt}\n",
    "    '''}) \n",
    "  \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=1, #play with temp to get more factual responses\n",
    "        max_tokens=777,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    # Parsing out what we want to see displayed in the chatbot response\n",
    "    assistant_content = response['choices'][0]['message']['content']\n",
    "\n",
    "    # Visual check for testing purposes\n",
    "    print(response) \n",
    "\n",
    "    # Append messages memory  \n",
    "    messages.append(({'role':'assistant', 'content': assistant_content}))\n",
    "\n",
    "    # Saving content to a text file\n",
    "    # Open a file in write mode, 'a' argument serves as appending to\n",
    "    with open('saved_chat.txt', 'a') as file:\n",
    "        file.write(f'''\n",
    "        User: {prompt}\n",
    "        \n",
    "        Response: {assistant_content}\n",
    "        ''')\n",
    "\n",
    "    \n",
    "    #add token counter and a way to handle hitting token limit. Prob summarizing previous assistant/student info into 2-4 sentences\n",
    "    \n",
    "    return assistant_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "145bf262-73e3-46cd-8b18-815b57f83de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://49d8de929bfdcc90ec.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://49d8de929bfdcc90ec.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7kPAuW9tnNUTyHzVdP9hy1qs6yr3j\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1691294216,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"WALKTHROUGH:\\nThe `sort_values()` function is a method in pandas that allows you to sort a DataFrame or a Series by one or more columns. It arranges the rows of the DataFrame or Series in ascending or descending order based on the specified column(s). \\n\\nThe basic syntax for `sort_values()` is:\\n\\n```\\nDataFrame.sort_values(by, axis=0, ascending=True, inplace=False, ignore_index=False, na_position='last')\\n```\\n\\nHere's a breakdown of the parameters:\\n\\n- `by`: Specifies the column(s) or label(s) by which you want to sort the data. It can be a single column name or a list of column names. You can also sort by multiple columns by passing a list of column names in the desired sorting order.\\n- `axis`: Specifies the axis along which to sort. By default, it is set to 0, which sorts the rows. You can set it to 1 to sort the columns.\\n- `ascending`: Specifies whether to sort in ascending or descending order. By default, it is set to True, which sorts in ascending order. Set it to False to sort in descending order.\\n- `inplace`: Specifies whether to modify the DataFrame directly or return a sorted copy. If set to True, it modifies the original DataFrame. If set to False (the default), it returns a sorted copy.\\n- `ignore_index`: Specifies whether to reset the index after sorting. By default, it is set to False. If set to True, the resulting DataFrame will have a new index.\\n- `na_position`: Specifies where missing values (NaN values) should be placed during sorting. It can be set to 'last' (default), 'first', or 'raise'. \\n\\nHere's an example usage of `sort_values()`:\\n\\n``` python\\nimport pandas as pd\\n\\n# Creating a sample DataFrame\\ndata = {'Name': ['Alice', 'Bob', 'Charlie', 'David'],\\n        'Age': [25, 30, 22, 27],\\n        'Salary': [50000, 60000, 45000, 55000]}\\n\\ndf = pd.DataFrame(data)\\n\\n# Sort the DataFrame by the 'Salary' column in descending order\\nsorted_df = df.sort_values(by='Salary', ascending=False)\\n\\nprint(sorted_df)\\n```\\n\\nIn this example, we create a DataFrame with columns 'Name', 'Age', and 'Salary'. We use `sort_values()` to sort the\"\n",
      "      },\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 1552,\n",
      "    \"completion_tokens\": 500,\n",
      "    \"total_tokens\": 2052\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7kPBYha4mfK6zconWTTBtWErydvvE\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1691294256,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Hello! How can I assist you today with your data science questions or topics?\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 2061,\n",
      "    \"completion_tokens\": 16,\n",
      "    \"total_tokens\": 2077\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# call the function with Gradio ChatInterface function\n",
    "demo = gr.ChatInterface(\n",
    "    fn=ai_tutor, \n",
    "    submit_btn = 'Submit', \n",
    "    retry_btn = 'Retry',\n",
    "    clear_btn = 'Clear',\n",
    "    title = 'AI Tutor',\n",
    "    #add a save convo button that can create html file that can then be printed like a pdf later\n",
    "    #and analyzed later/flash cards made\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e722b690-1a94-41e3-8d3c-03d63b487567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a save bot that can parse ai_tutor responses and save the data in the appropriate format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c51c51f4-b8e9-4f3e-932a-8da1fe2404ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_chat function\n",
    "# dynamically saves chat to a txt file in a certain format that can be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a50c30d-73e5-4761-8eee-a90024bb8163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when given a csv with just questions column, it should be able to fill out the walkthrough, hints, and solutions part of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c2f718-b8e8-4648-a7ec-4a3df19e8f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7kOzNFMyWpN2JJQm0zTURpmmfHknK\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1691293501,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Hello! How can I assist you today with your data science questions or topics?\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 361,\n",
      "    \"completion_tokens\": 16,\n",
      "    \"total_tokens\": 377\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7kOzZOhV2yvvIlutcKh42Fp2uNyPW\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1691293513,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"To identify the peak points in a time series chart with a lot of ups and downs, you can use various techniques. Here's a common approach:\\n\\n1. Smooth the data: Apply a moving average or a smoothing function to reduce noise and highlight the underlying trends in the data. This can make it easier to identify the peaks.\\n\\n2. Differentiate the data: Take the derivative of the time series data to find its rate of change at each point. Peaks in the derivative correspond to points where the original time series has a sharp increase or decrease.\\n\\n3. Find local maxima: Look for points where the value is higher than its neighboring points. These are potential peak points in the time series.\\n\\n4. Apply thresholding: Set a threshold value above which points are considered peaks. This threshold can be based on the standard deviation or a percentage of the maximum value in the time series.\\n\\n5. Validate the peaks: Check if the identified peak points meet your criteria or specifications. You may want to consider the amplitude, duration, or other characteristics of the peaks.\\n\\nRemember, the choice of technique may vary depending on the specific characteristics of your data. It's always a good idea to visually inspect the results and manually verify the identified peak points in the context of your analysis.\\n\\nLet me know if you have any further questions or need more detailed guidance.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 406,\n",
      "    \"completion_tokens\": 270,\n",
      "    \"total_tokens\": 676\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7kP1uVJzXj3iw1wSx8SklQVIiW47b\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1691293658,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"QUESTION:\\nWhat is the overall friend acceptance rate by date? Your output should have the rate of acceptances by the date the request was sent. Order by the earliest date to latest.\\n\\nAssume that each friend request starts by a user sending (i.e., user_id_sender) a friend request to another user (i.e., user_id_receiver) that's logged in the table with action = 'sent'. If the request is accepted, the table logs action = 'accepted'. If the request is not accepted, no record of action = 'accepted' is logged.\\n\\nHINT:\\nTo calculate the friend acceptance rate by date, you will need to count the number of accepted requests for each date and divide it by the total number of sent requests for that date. You can use SQL queries or pandas library in Python to accomplish this task.\\n\\nWALKTHROUGH:\\nThere are a few steps involved in calculating the friend acceptance rate by date:\\n\\n1. Retrieve the data: You need to retrieve the relevant data that includes the sender ID, receiver ID, action, and the date the request was sent from the table.\\n\\n2. Filter the data: Filter the data to include only the records where the action is either 'sent' or 'accepted'.\\n\\n3. Group the data: Group the data by the date the request was sent.\\n\\n4. Count the number of sent and accepted requests: Count the number of records where the action is 'sent' and 'accepted' for each date.\\n\\n5. Calculate the acceptance rate: Divide the count of accepted requests by the count of sent requests for each date to get the acceptance rate.\\n\\n6. Sort the results: Sort the results in ascending order based on the date.\\n\\nSOLUTION:\\nHere's an example solution using pandas library in Python:\\n\\n```python\\nimport pandas as pd\\n\\n# Assuming you have a DataFrame called 'data' with the relevant columns\\n\\n# Filter the data to include only the records with action 'sent' or 'accepted'\\nfiltered_data = data[data['action'].isin(['sent', 'accepted'])]\\n\\n# Group the data by the date the request was sent and count the number of sent and accepted requests\\ngrouped_data = filtered_data.groupby('date').agg({'action': 'count'}).rename(columns={'action': 'sent_requests'})\\ngrouped_data['accepted_requests'] = filtered_data[filtered_data['action'] == 'accepted'].groupby('date')['action'].count()\\n\\n# Calculate the acceptance rate by dividing the count of accepted\"\n",
      "      },\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 796,\n",
      "    \"completion_tokens\": 500,\n",
      "    \"total_tokens\": 1296\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7kP2mMOOfMlhDSFrTrok6jJ5zjzjn\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1691293712,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Sure! Here's the continuation of the solution:\\n\\n```python\\n# Calculate the acceptance rate by dividing the count of accepted requests by the count of sent requests\\ngrouped_data['acceptance_rate'] = grouped_data['accepted_requests'] / grouped_data['sent_requests']\\n\\n# Sort the results in ascending order based on the date\\nsorted_data = grouped_data.sort_values('date')\\n\\n# Print the resulting DataFrame with the acceptance rate by date\\nprint(sorted_data)\\n```\\n\\nThis solution assumes that you have a DataFrame called 'data' with the relevant columns, including 'date' for the date the request was sent and 'action' for the action taken (either 'sent' or 'accepted').\\n\\nEDGE CASES:\\n- If there are no 'sent' requests on a particular date, the acceptance rate will be NaN (not a number) for that date. You can handle this by replacing NaN values with 0 or considering such cases separately.\\n- If there is no 'accepted' action logged for a 'sent' request, it means the request was not accepted. In this case, the count of accepted requests will be 0 for that date.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 1305,\n",
      "    \"completion_tokens\": 232,\n",
      "    \"total_tokens\": 1537\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# categorization bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c6edaf-402e-4b57-8ad9-620831a68992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220f43a-ee23-4614-a697-0eb6ff55ac62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6e021-b160-4b92-b4ea-ab5bc6c5f0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
