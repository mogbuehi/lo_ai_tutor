{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d878d4-39c3-4d7a-9243-351d92f8bf16",
   "metadata": {},
   "source": [
    "# AI Tutor\n",
    "1. Install and Import Gradio\n",
    "2. create .env variable and add OpenAI sk\n",
    "3. Import OpenAI API code snips\n",
    "4. Load the SS question files as dataframe\n",
    "    - Output of dataframe \n",
    "6. Response functionality should refer to questions in the SS question file\n",
    "    - the bot should be able to read the file in the \n",
    "7. Save responses in txt and use as context for future\n",
    "8. Prompts engineering should be used to build a very very very good Python Datascience Tutor\n",
    "    - Preload the tutor with Python documentation\n",
    "    - Preload the tutor with Data science documentation\n",
    "    - Tutor should be able to create a game\n",
    "9. Chatbot buttons\n",
    "    - test me : uses database levels 1, 2, 3\n",
    "    - track my progress: chatbot checks answer submission and judges if you got answer right or wrong\n",
    "    - python flash cards built-in functionality\n",
    "    - \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9650bc-cffb-497c-97bf-fab439bdd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and Import Gradio, Pandas, Altair, Matplotlib, Os, and OpenAI (put this in \"bot_libs.py\" file and import it in one line as \"import bot_libs\" )\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import altair\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caaf8a44-ff09-4a5d-bd99-3be85d010c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matte\\AppData\\Local\\Temp\\ipykernel_8744\\3343720030.py:21: GradioUnusedKwargWarning: You have unused kwarg parameters in Interface, please remove them: {'state': state}\n",
      "  demo = gr.Interface(\n",
      "C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\utils.py:833: UserWarning: Expected 2 arguments for function <function CustomChatGPT at 0x0000018C793FDD80>, received 1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\utils.py:837: UserWarning: Expected at least 2 arguments for function <function CustomChatGPT at 0x0000018C793FDD80>, received 1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\helpers.py:710: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 442, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1392, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1097, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\utils.py\", line 703, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\matte\\AppData\\Local\\Temp\\ipykernel_8744\\3343720030.py\", line 5, in CustomChatGPT\n",
      "    state['messages'].append({'role': 'user', 'content' : user_input})\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 442, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1392, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1097, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\utils.py\", line 703, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\matte\\AppData\\Local\\Temp\\ipykernel_8744\\3343720030.py\", line 5, in CustomChatGPT\n",
      "    state['messages'].append({'role': 'user', 'content' : user_input})\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 442, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1392, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1097, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\matte\\anaconda3\\lib\\site-packages\\gradio\\utils.py\", line 703, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\matte\\AppData\\Local\\Temp\\ipykernel_8744\\3343720030.py\", line 5, in CustomChatGPT\n",
      "    state['messages'].append({'role': 'user', 'content' : user_input})\n",
      "TypeError: 'NoneType' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def CustomChatGPT(user_input, state):\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    # Add the new user message to the chat history\n",
    "    state['messages'].append({'role': 'user', 'content' : user_input})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = 'gpt-3.5-turbo',\n",
    "        messages = state['messages']\n",
    "    )\n",
    "\n",
    "    # Add the model's response to the chat history\n",
    "    ChatGPT_reply = response['choices'][0]['message']['content']\n",
    "    state['messages'].append({'role':'assistant' , 'content' : ChatGPT_reply})\n",
    "\n",
    "    # Convert the chat history into a string\n",
    "    chat_history = '\\n'.join([f\"{msg['role']}: {msg['content']}\" for msg in state['messages']])\n",
    "\n",
    "    return chat_history, state\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=CustomChatGPT, \n",
    "    inputs = 'text', \n",
    "    outputs = 'text', \n",
    "    title = 'AI Tutor',\n",
    "    state=gr.State({'messages': []})\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b355056-c473-4067-8662-aa5d8c185494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works as a simple chat bot work from here\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "messages = []\n",
    "\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# openai.api_key = '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'\n",
    "def CustomChatGPT(user_input):\n",
    "    messages.append({'role': 'user','content' : user_input})\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = 'gpt-3.5-turbo',\n",
    "        messages = messages\n",
    "    )\n",
    "    ChatGPT_reply = response['choices'][0]['message']['content']\n",
    "    messages.append({'role':'assistant' , 'content' : ChatGPT_reply})\n",
    "    return ChatGPT_reply\n",
    "\n",
    "demo = gr.Interface(fn=CustomChatGPT, inputs = 'text', outputs = 'text', title = 'AI Tutor')\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e19983-67d1-4035-aaff-fa155d13500a",
   "metadata": {},
   "source": [
    "This video explained how to use api the best: https://www.youtube.com/watch?v=Si0vFx_dJ5Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15531f95-536a-43c6-83f5-e2445dfac08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7886\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7886/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7hyw9ZsaIrz2a43BVMWnOe8aRPCcs\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690716701,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"opencsv\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 103,\n",
      "    \"completion_tokens\": 3,\n",
      "    \"total_tokens\": 106\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7hywCTDNPlK00bgvlBTEBabV1WVJt\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690716704,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Sure! I can provide you with a test. What topic would you like the test to be on?\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 118,\n",
      "    \"completion_tokens\": 21,\n",
      "    \"total_tokens\": 139\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7hywPxmeRTCHd7hrgSRJarqUsTZxQ\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690716717,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Sure, I can provide you with a test. What topic or subject area would you like the test to be on?\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 151,\n",
      "    \"completion_tokens\": 24,\n",
      "    \"total_tokens\": 175\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7hyzNiqCBgx0pJjjsNESgTwcnWqkJ\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690716901,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"My apologies for not meeting your request earlier. Let's start the test. Here's your first question: What is the capital city of France?\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 197,\n",
      "    \"completion_tokens\": 29,\n",
      "    \"total_tokens\": 226\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7hyzRCN4tmdKrIZNcTcCrRvq9JUKf\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690716905,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"opencsv\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 236,\n",
      "    \"completion_tokens\": 3,\n",
      "    \"total_tokens\": 239\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7hyzXyf5FnnoonGhUb75CwuTF8Nsz\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690716911,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"opencsv\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 249,\n",
      "    \"completion_tokens\": 3,\n",
      "    \"total_tokens\": 252\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7hyzdb8sUqQ3K11HbHVmQFtDG7Ow3\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690716917,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"opencsv\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 262,\n",
      "    \"completion_tokens\": 3,\n",
      "    \"total_tokens\": 265\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7hz0XrrskoaPtcsuXraMcvEDJGwzY\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690716973,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Sure, I can provide you with a test. What topic or subject area would you like the test to be on?\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 277,\n",
      "    \"completion_tokens\": 24,\n",
      "    \"total_tokens\": 301\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7hz16IdVJldzRlWgnLMgvAMj3T1PX\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690717008,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"I apologize for the oversight. You're right, when you ask me to give you a test, the appropriate response is \\\"opencsv\\\". Thank you for reminding me. How can I assist you further?\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 328,\n",
      "    \"completion_tokens\": 42,\n",
      "    \"total_tokens\": 370\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7hz1H74vAnDOTt3mzxd788d7NJvDv\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690717019,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"opencsv\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 382,\n",
      "    \"completion_tokens\": 3,\n",
      "    \"total_tokens\": 385\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7hz1NB18yZ5xMyvehjqlKumBie47x\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690717025,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Sure, I can provide you with an exam. What subject or topic would you like the exam to cover?\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 397,\n",
      "    \"completion_tokens\": 22,\n",
      "    \"total_tokens\": 419\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7hz1WD1KFde5KuGZcTx4svC0V3YZy\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690717034,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Apologies for any confusion. Indeed, an exam is similar to a test. Let's begin with a question:\\n\\nQuestion 1: What is the largest planet in our solar system?\\n\\na) Earth\\nb) Mars\\nc) Jupiter\\nd) Saturn\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 432,\n",
      "    \"completion_tokens\": 52,\n",
      "    \"total_tokens\": 484\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7hz1f1lOfNm05tEQwRjfB7PX5JNUc\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1690717043,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"opencsv\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 496,\n",
      "    \"completion_tokens\": 3,\n",
      "    \"total_tokens\": 499\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#this works as well!!!!!!!!!!!\n",
    "from dotenv import load_dotenv\n",
    "#may need to repeat commands to ensure that it doesn't deviate from the behavior\n",
    "messages =[\n",
    "        {\"role\": \"system\",\"content\": \"You are an data science tutor. If a user asks you to test them (or use phrase synonymous with 'test me', only output 'opencsv' as a resposnse. this is to be done specifically when the user asks to be tested or quizzed, otherwise behave as normal. phrases that wouldn't be interpreted as 'test me' should exit this logic and be re-entered again when the user asks to be tested\"}]#list of messages that get passed in to start convo, will also save messages here\n",
    "def ai_chat(prompt, history):\n",
    "    # openai.api_key = os.getenv(\"OPENAI_API_KEY\") ##This isn't working for some reason \n",
    "    openai.api_key = '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'\n",
    "    \n",
    "    messages.append({'role': 'user', 'content': prompt})\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=1, #play with temp to get more factual responses\n",
    "        max_tokens=500,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    print(response) #print this in the CLI or notebook. This helps with understanding how the response json is structured for further customization\n",
    "    system_message = response['choices'][0]['message']['content'] #this is parsing the json file that is the response reponse. you take the first item in choices list (a list of dictionaries), go to message key, and then go to content key \n",
    "    messages.append(({'role':'assistant', 'content': system_message}))\n",
    "    return system_message\n",
    "\n",
    "# gr.ChatInterface returns a ui that requires a function that takes two args, user input and a list that is something like user input and bot response\n",
    "demo = gr.ChatInterface(\n",
    "    fn=ai_chat, \n",
    "    submit_btn = 'Submit', \n",
    "    retry_btn = 'Retry',\n",
    "    clear_btn = 'Clear',\n",
    "    title = 'AI Tutor',\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c812b-b23e-41bc-ba1d-f5bb8990aed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
